{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:24:03.732798Z","iopub.execute_input":"2025-09-20T17:24:03.733341Z","iopub.status.idle":"2025-09-20T17:24:03.737739Z","shell.execute_reply.started":"2025-09-20T17:24:03.733317Z","shell.execute_reply":"2025-09-20T17:24:03.737141Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# RAG (Retrieval Agumented Generation)\n\nRAG kavramı altında, genel akış, uçtan uca çalışma prensipleri, farklı chunking (parçalama) yaklaşımları ve bu yaklaşımların olası sonuçları anlatılacaktır. ","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers scikit-learn numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:24:05.111245Z","iopub.execute_input":"2025-09-20T17:24:05.111543Z","iopub.status.idle":"2025-09-20T17:25:28.276344Z","shell.execute_reply.started":"2025-09-20T17:24:05.111519Z","shell.execute_reply":"2025-09-20T17:25:28.275318Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Aşağıda, karmaşık kütüphaneler (LangChain, LlamaIndex vb.) olmadan, sadece temel sentence-transformers ve scikit-learn kullanarak uçtan uca bir RAG yapısını gösteren bir Python kodu bulunmaktadır. Her adım, kodun hemen altında örnek çıktılarla açıklanmıştır.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport textwrap\n\n# --- 1. Adım: BİLGİ KAYNAĞI (KNOWLEDGE BASE) ---\nKNOWLEDGE_BASE = \"\"\"\nTürkiye’de yapay zekâ alanında çalışan pek çok isim, farklı uzmanlık alanları ve yaklaşımlarıyla ülkenin bu konudaki bilgi birikimini\nve uygulama kapasitesini zenginleştirmektedir. Bu isimlerden biri olan Cem Say, Boğaziçi Üniversitesi’nde bilgisayar mühendisliği alanında\ngörev yapmakta olup, özellikle teorik bilgisayar bilimi, kuantum hesaplama ve yapay zekâ felsefesi üzerine çalışmalar yürütmektedir.\nHem akademik hem de popüler düzeyde yazdığı eserlerle, yapay zekâ konusunu toplumun geniş kesimlerine ulaştırma konusunda öncü isimler\narasında yer almaktadır.\nAytül Erçil, bilgisayarlı görü ve yapay zekâ temelli görüntü işleme alanlarında uzmanlaşmış bir akademisyen ve girişimcidir. \nSabancı Üniversitesi’nde uzun yıllar akademik faaliyetlerde bulunmuş, daha sonra kurduğu teknoloji şirketleri aracılığıyla \nyapay zekâ tabanlı perakende çözümleri geliştirmiştir. Görüntü tanıma, raf düzenlemesi ve veri analitiği gibi konularda yapay \nzekâyı ticari uygulamalara başarıyla dönüştürmüştür.\nTürker Tekin Ergüzel, yapay zekânın sağlık bilişimi ile kesişiminde çalışan bir başka önemli isimdir. Özellikle EEG ve MRI \ngibi nörogörüntüleme verilerinin derin öğrenme yöntemleriyle analiz edilmesi, psikiyatrik hastalıkların erken tanısında kullanılabilecek \nmodeller geliştirilmesi gibi konularda uluslararası düzeyde çalışmalar yürütmektedir. Yürüttüğü projeler, yapay zekânın klinik karar destek \nsistemlerine entegrasyonunu mümkün kılmaktadır.\nŞadi Evren Şeker, veri bilimi, makine öğrenmesi ve açıklanabilir yapay zekâ alanlarında çalışan bir akademisyendir. \nAkademik yayınlarının yanı sıra, yapay zekâ projelerinin kamu politikaları, etik ilkeler ve sorumlu inovasyonla uyumlu şekilde \ngeliştirilmesine yönelik çalışmalar da yürütmektedir. Aynı zamanda çeşitli projelerde danışmanlık yapmakta, yapay zekâ sistemlerinin \ndoğru tasarlanması ve güvenli biçimde uygulanması için model geliştirmektedir.\nCansu Canca, felsefe kökenli bir yapay zekâ etikçisidir. Yapay zekâ sistemlerinin etik, toplumsal ve hukuksal etkileri üzerine çalışmalar \nyürütmektedir. Özellikle teknoloji şirketlerinin geliştirdiği yapay zekâ çözümlerine etik gözetim ve değerlendirme çerçeveleri kazandırmak için \nçok sayıda proje yürütmüş, bu alanda uluslararası düzeyde etkinlik kazanmıştır.\nBu isimlerin dışında, Türkiye Yapay Zekâ İnisiyatifi çatısı altında birçok akademisyen, girişimci ve mühendis, yapay zekâ ekosistemine \nkatkı sunmaktadır. Üniversitelerde kurulan yapay zekâ araştırma merkezleri, teknoparklarda geliştirilen yeni nesil girişimler ve kamu \ndestekli projeler sayesinde Türkiye’de yapay zekâ alanında hem nitelik hem nicelik bakımından güçlü bir insan kaynağı oluşmaktadır. Her biri \nkendi alanında özgün katkılar sunan bu uzmanlar, Türkiye’nin yapay zekâ geleceğini şekillendiren önemli aktörlerdir.\n\"\"\"\n\n# --- 2. Adım: PARÇALAMA (CHUNKING) ---\ndef chunk_text(text: str) -> list[str]:\n    \"\"\"Metni boş satırlara (paragraflara) göre böler.\"\"\"\n    chunks = [p.strip() for p in text.split('\\n') if p.strip()]\n    return chunks\n\n# --- 3. Adım: GÖMME (EMBEDDING) ---\ndef get_embeddings(chunks, model):\n    \"\"\"Metin parçalarını vektörlere dönüştürür.\"\"\"\n    embeddings = model.encode(chunks, show_progress_bar=True)\n    return embeddings\n\n# --- 4. Adım: ARAMA ve GETİRME (RETRIEVAL) ---\n# Fonksiyonu daha verimli hale getirdik: Modeli parametre olarak alıyor.\ndef find_most_similar_chunks(query, document_embeddings, chunks, model, top_k=1):\n    \"\"\"Sorguya en benzer metin parçalarını bulur.\"\"\"\n    query_embedding = model.encode([query])\n    \n    similarities = cosine_similarity(query_embedding, document_embeddings)[0]\n    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n    \n    relevant_chunks = [chunks[i] for i in top_k_indices]\n    return relevant_chunks\n\n# --- 5. Adım: CEVAP ÜRETME (GENERATION) ---\ndef generate_answer(query, context):\n    \"\"\"Bağlam ve sorguyu kullanarak bir cevap üretir (Mock LLM).\"\"\"\n    prompt = f\"\"\"\nAşağıdaki bağlamı kullanarak verilen soruyu cevapla. Cevabını sadece sana verilen bağlama dayandır.\n\nBağlam:\n---\n{context}\n---\n\nSoru: {query}\n\nCevap:\n\"\"\"\n    print(\"--- LLM'e Gönderilen Nihai Prompt ---\")\n    print(prompt)\n    \n    mock_response = f\"Sağlanan bilgilere göre, '{query}' sorusunun cevabı şudur: {context}\"\n    return mock_response\n\n# --- UÇTAN UCA RAG SÜRECİNİ ÇALIŞTIRMA ---\nif __name__ == \"__main__\":\n    # Embedding modelini bir kez en başta yüklüyoruz.\n    embedding_model = SentenceTransformer('dbmdz/bert-base-turkish-cased')\n\n    print(\"--- 1. ve 2. Adım: Bilgi Kaynağını Parçalama (Chunking) ---\")\n    chunks = chunk_text(KNOWLEDGE_BASE)\n    print(f\"{len(chunks)} adet chunk (paragraf) oluşturuldu.\")\n    print(\"-\" * 50)\n\n    print(\"--- 3. Adım: Metin Parçalarını Gömme (Embedding) ---\")\n    document_embeddings = get_embeddings(chunks, embedding_model)\n    print(f\"Her bir chunk için {document_embeddings.shape[1]} boyutlu bir vektör oluşturuldu.\")\n    print(\"-\" * 50)\n    \n    \n    user_query = \"kimler yapay zeka çalışır?\"\n\n\n\n    \n    print(f\"Kullanıcı Sorgusu: '{user_query}'\")\n    print(\"-\" * 50)\n    \n    print(\"--- 4. Adım: İlgili Bilgiyi Arama ve Getirme (Retrieval) ---\")\n    context_chunks = find_most_similar_chunks(user_query, document_embeddings, chunks, embedding_model)\n    context_for_llm = \"\\n\\n\".join(context_chunks)\n    print(\"Bulunan En Alakalı Bağlam (Context):\")\n    print(context_for_llm)\n    print(\"-\" * 50)\n    \n    print(\"--- 5. Adım: Nihai Cevabı Üretme (Generation) ---\")\n    final_answer = generate_answer(user_query, context_for_llm)\n    print(\"-\" * 50)\n    \n    print(\"✅ Uçtan Uca RAG Süreci Tamamlandı!\")\n    print(\"Nihai Cevap:\", final_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:25:28.278300Z","iopub.execute_input":"2025-09-20T17:25:28.278968Z","iopub.status.idle":"2025-09-20T17:26:09.499546Z","shell.execute_reply.started":"2025-09-20T17:25:28.278938Z","shell.execute_reply":"2025-09-20T17:26:09.498893Z"}},"outputs":[{"name":"stderr","text":"2025-09-20 17:25:42.628023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758389142.853370      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758389142.920122      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e559c4f464424ff5b6c486b25ca543ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94310d59894b418dab872f9c44e73353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563d205859394262bc1cf82ba3d992f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec66f2b612c400aaf0a5d1a29e93cca"}},"metadata":{}},{"name":"stdout","text":"--- 1. ve 2. Adım: Bilgi Kaynağını Parçalama (Chunking) ---\n24 adet chunk (paragraf) oluşturuldu.\n--------------------------------------------------\n--- 3. Adım: Metin Parçalarını Gömme (Embedding) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c42e77401142bf92643f909e243ec5"}},"metadata":{}},{"name":"stdout","text":"Her bir chunk için 768 boyutlu bir vektör oluşturuldu.\n--------------------------------------------------\nKullanıcı Sorgusu: 'kimler yapay zeka çalışır?'\n--------------------------------------------------\n--- 4. Adım: İlgili Bilgiyi Arama ve Getirme (Retrieval) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ab1c42ebab470db2b9e331b301db91"}},"metadata":{}},{"name":"stdout","text":"Bulunan En Alakalı Bağlam (Context):\nŞadi Evren Şeker, veri bilimi, makine öğrenmesi ve açıklanabilir yapay zekâ alanlarında çalışan bir akademisyendir.\n--------------------------------------------------\n--- 5. Adım: Nihai Cevabı Üretme (Generation) ---\n--- LLM'e Gönderilen Nihai Prompt ---\n\nAşağıdaki bağlamı kullanarak verilen soruyu cevapla. Cevabını sadece sana verilen bağlama dayandır.\n\nBağlam:\n---\nŞadi Evren Şeker, veri bilimi, makine öğrenmesi ve açıklanabilir yapay zekâ alanlarında çalışan bir akademisyendir.\n---\n\nSoru: kimler yapay zeka çalışır?\n\nCevap:\n\n--------------------------------------------------\n✅ Uçtan Uca RAG Süreci Tamamlandı!\nNihai Cevap: Sağlanan bilgilere göre, 'kimler yapay zeka çalışır?' sorusunun cevabı şudur: Şadi Evren Şeker, veri bilimi, makine öğrenmesi ve açıklanabilir yapay zekâ alanlarında çalışan bir akademisyendir.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Chunking Yaklaşımları\nAşağıda, en yaygın ve önemli chunking stratejilerini (Recursive, Semantic, ve Layout-Aware) karşılaştıran, Python kodu, adım adım çıktılar ve görsel temsiller içeren detaylı bir rehber kod verilmiştir. ","metadata":{}},{"cell_type":"markdown","source":"\n## STRATEJİ: Recursive Character Splitting\n\nParametreler: chunk_size=200, chunk_overlap=20\n\n--- Oluşturulan Chunk'lar ---\nCHUNK 1:\n\"QuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon\"\n(Uzunluk: 198 karakter)\n\nCHUNK 2:\n\"sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\"\n(Uzunluk: 111 karakter)\n\n... (diğer chunk'lar) ...\n\n\n\n## STRATEJİ: Semantic Chunking\n\nParametreler: similarity_breakpoint_threshold=0.45\nAra Çıktı: Cümleler Arası Benzerlik Skorları \n  Cümle 1 <-> Cümle 2 Benzerlik: 0.75\n  Cümle 2 <-> Cümle 3 Benzerlik: 0.68\n  Cümle 3 <-> Cümle 4 Benzerlik: 0.31  <-- KIRILMA NOKTASI BURADA TESPİT EDİLDİ\n  Cümle 4 <-> Cümle 5 Benzerlik: 0.65\n  Cümle 5 <-> Cümle 6 Benzerlik: 0.78\n  Cümle 6 <-> Cümle 7 Benzerlik: 0.55\n\n\n--- Oluşturulan Chunk'lar ---\n\n* CHUNK 1:\n\"QuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\"\n(Uzunluk: 312 karakter)\n\n* CHUNK 2:\n\"Projenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\"\n(Uzunluk: 494 karakter)\n\n\n\n## STRATEJİ: Layout-Aware Chunking (Demo)\n\nPrensip: '##' ile başlayan her başlık yeni bir chunk başlatır.\n\n--- Oluşturulan Chunk'lar ---\n* CHUNK 1:\n\"Projenin Amacı\n\nQuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\"\n(Uzunluk: 331 karakter)\n\n* CHUNK 2:\n\"Teknik Detaylar\n\nProjenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\"\n(Uzunluk: 512 karakter)\n\n\n\n## STRATEJİLERİN KARŞILAŞTIRMALI ÖZET TABLOSU\n\n| Kriter| Recursive Splitting         | Semantic Chunking         | Layout-Aware Chunking     |\n|-------------------|-----------------------------|---------------------------|---------------------------|\n| Temel Fikir | Yapısal hiyerarşiye göre böl| Anlam akışına göre böl    | Doküman formatına göre böl|\n| Sonuç Kalitesi| Orta - Anlamı bölebilir     | Yüksek - Konu bütünlüğü   | Çok Yüksek (Yapısal Metinde)|\n| Hesaplama Yükü| Düşük                       | Orta-Yüksek               | Düşük-Orta                |\n| Ne Zaman Kullanmalı?| Hızlı prototip, baseline    | Anlatısal metinler        | Raporlar, dokümantasyonlar|\n","metadata":{}},{"cell_type":"code","source":"import re\nimport textwrap\nimport numpy as np\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# --- 1. GENEL AYARLAR VE VERİ ---\n\n# Tüm stratejilerde kullanılacak örnek metin\nSAMPLE_TEXT = \"\"\"\nQuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\nfdsafjsdalfj ljfasldfkjs dafklsa.\nTeknik Detaylar Projenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\n\"\"\"\n\n# Layout-Aware stratejisi için Markdown formatlı metin\nMARKDOWN_SAMPLE_TEXT = \"\"\"\n## Projenin Amacı\n\nQuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\n\n## Teknik Detaylar\n\nProjenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\n\"\"\"\n\n# Embedding modelini bir kez yükleyip tüm fonksiyonlarda kullanacağız.\nEMBEDDING_MODEL = SentenceTransformer('google/flan-t5-base')\n\n\n# --- 2. CHUNKING STRATEJİSİ FONKSİYONLARI ---\n\ndef run_recursive_chunking(text: str, chunk_size: int = 200, chunk_overlap: int = 20) -> list[str]:\n    \"\"\"\n    LangChain'in RecursiveCharacterTextSplitter'ını kullanarak metni parçalar.\n    \"\"\"\n    print(f\"Parametreler: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    return splitter.split_text(text)\n\n\ndef run_semantic_chunking(text: str, model, threshold: float = 0.45) -> tuple[list[str], list[float]]:\n    \"\"\"\n    Metni cümlelere ayırır ve anlamsal benzerlik eşiğine göre chunk'lar oluşturur.\n    Ara çıktı olarak benzerlik skorlarını da döndürür.\n    \"\"\"\n    print(f\"Parametreler: similarity_breakpoint_threshold={threshold}\")\n    # Metni cümlelere böl (basit bir regex ile)\n    sentences = re.split(r'(?<=[.!?])\\s+', text.replace(\"\\n\", \" \").strip())\n    \n    # Her cümlenin embedding'ini hesapla\n    embeddings = model.encode(sentences)\n    \n    # Ardışık cümleler arası kosinüs benzerliğini hesapla\n    similarities = []\n    for i in range(len(sentences) - 1):\n        emb1 = embeddings[i].reshape(1, -1)\n        emb2 = embeddings[i+1].reshape(1, -1)\n        sim = cosine_similarity(emb1, emb2)[0][0]\n        similarities.append(sim)\n        \n    # Eşik değerinin altına düşen yerleri kırılma noktası olarak belirle\n    breakpoint_indices = [i + 1 for i, sim in enumerate(similarities) if sim < threshold]\n    \n    # Kırılma noktalarına göre chunk'ları oluştur\n    chunks = []\n    start_index = 0\n    for break_index in breakpoint_indices:\n        chunk = \" \".join(sentences[start_index:break_index])\n        chunks.append(chunk)\n        start_index = break_index\n    chunks.append(\" \".join(sentences[start_index:])) # Son chunk'ı ekle\n    \n    return chunks, similarities\n\n\ndef run_layout_aware_chunking_demo(markdown_text: str) -> list[str]:\n    \"\"\"\n    Layout-Aware chunking prensibini basitçe Markdown başlıklarına göre bölerek gösterir.\n    \"\"\"\n    print(\"Prensip: '##' ile başlayan her başlık yeni bir chunk başlatır.\")\n    # Gerçek bir uygulamada LangChain'in MarkdownHeaderTextSplitter'ı kullanılır.\n    # Biz burada prensibi göstermek için basitçe bölüyoruz.\n    return markdown_text.strip().split('## ')[1:] # İlk eleman boş olacağı için atlıyoruz\n\n\n# --- 3. GÖRSELLEŞTİRME VE SONUÇ GÖSTERİMİ ---\n\ndef display_results(strategy_name: str, chunks: list, extra_info=None):\n    \"\"\"\n    Her stratejinin sonucunu standart bir formatta ekrana basar.\n    \"\"\"\n    print(\"\\n\" + \"=\"*50)\n    print(f\"STRATEJİ: {strategy_name}\")\n    print(\"=\"*50)\n    \n    if extra_info:\n        if \"similarities\" in extra_info:\n            print(\"--- Ara Çıktı: Cümleler Arası Benzerlik Skorları ---\")\n            for i, sim in enumerate(extra_info[\"similarities\"]):\n                print(f\"  Cümle {i+1} <-> Cümle {i+2} Benzerlik: {sim:.2f}\")\n            print(\"-\" * 20)\n\n    print(\"\\n--- Oluşturulan Chunk'lar ---\")\n    for i, chunk in enumerate(chunks):\n        print(f\"CHUNK {i+1}:\")\n        print(f'\"{chunk.strip()}\"')\n        print(f\"(Uzunluk: {len(chunk)} karakter)\\n\")\n    print(\"\\n\")\n\n\ndef display_comparison_table():\n    \"\"\"\n    Tüm stratejileri karşılaştıran bir tabloyu ekrana basar.\n    \"\"\"\n    header = \"| Kriter            | Recursive Splitting         | Semantic Chunking         | Layout-Aware Chunking     |\"\n    separator = \"|-------------------|-----------------------------|---------------------------|---------------------------|\"\n    row1 = \"| Temel Fikir       | Yapısal hiyerarşiye göre böl| Anlam akışına göre böl    | Doküman formatına göre böl| \"\n    row2 = \"| Sonuç Kalitesi    | Orta - Anlamı bölebilir     | Yüksek - Konu bütünlüğü   | Çok Yüksek (Yapısal Metinde)| \"\n    row3 = \"| Hesaplama Yükü    | Düşük                       | Orta-Yüksek               | Düşük-Orta                | \"\n    row4 = \"| Ne Zaman Kullanmalı?| Hızlı prototip, baseline    | Anlatısal metinler        | Raporlar, dokümantasyonlar| \"\n\n    print(\"=\"*80)\n    print(\"STRATEJİLERİN KARŞILAŞTIRMALI ÖZET TABLOSU\")\n    print(\"=\"*80)\n    print(header)\n    print(separator)\n    print(row1)\n    print(row2)\n    print(row3)\n    print(row4)\n    print(\"=\"*80)\n\n\n# --- 4. ANA PROGRAM AKIŞI ---\n\nif __name__ == \"__main__\":\n    # --- Strateji 1: Recursive ---\n    recursive_chunks = run_recursive_chunking(SAMPLE_TEXT)\n    display_results(\"Recursive Character Splitting\", recursive_chunks)\n\n    # --- Strateji 2: Semantic ---\n    semantic_chunks, similarities = run_semantic_chunking(SAMPLE_TEXT, EMBEDDING_MODEL)\n    display_results(\"Semantic Chunking\", semantic_chunks, extra_info={\"similarities\": similarities})\n\n    # --- Strateji 3: Layout-Aware ---\n    layout_chunks = run_layout_aware_chunking_demo(MARKDOWN_SAMPLE_TEXT)\n    display_results(\"Layout-Aware Chunking (Demo)\", layout_chunks)\n    \n    # --- Sonuç: Karşılaştırma Tablosu ---\n    display_comparison_table()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:26:33.587112Z","iopub.execute_input":"2025-09-20T17:26:33.587429Z","iopub.status.idle":"2025-09-20T17:26:40.515220Z","shell.execute_reply.started":"2025-09-20T17:26:33.587406Z","shell.execute_reply":"2025-09-20T17:26:40.514605Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0743c151fb140c0b28a92695e6950b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6d85cb3c684f5d91019032756e5cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30beb58497b4a3686f62fbd1bb3f495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e4c96c70e543f8aec18b9b8f42c0cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12214a7b5ee94e95a76eccccc063ea5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23fb63712ac4d58b7cd1eb8043a2348"}},"metadata":{}},{"name":"stdout","text":"Parametreler: chunk_size=200, chunk_overlap=20\n\n==================================================\nSTRATEJİ: Recursive Character Splitting\n==================================================\n\n--- Oluşturulan Chunk'lar ---\nCHUNK 1:\n\"QuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon\"\n(Uzunluk: 189 karakter)\n\nCHUNK 2:\n\"optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\"\n(Uzunluk: 116 karakter)\n\nCHUNK 3:\n\"fdsafjsdalfj ljfasldfkjs dafklsa.\"\n(Uzunluk: 33 karakter)\n\nCHUNK 4:\n\"Teknik Detaylar Projenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön\"\n(Uzunluk: 198 karakter)\n\nCHUNK 5:\n\"bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı\"\n(Uzunluk: 197 karakter)\n\nCHUNK 6:\n\"anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\"\n(Uzunluk: 55 karakter)\n\n\n\nParametreler: similarity_breakpoint_threshold=0.45\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f9181f42eb2455ebe9d6af77fa06ae6"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nSTRATEJİ: Semantic Chunking\n==================================================\n--- Ara Çıktı: Cümleler Arası Benzerlik Skorları ---\n  Cümle 1 <-> Cümle 2 Benzerlik: 0.87\n  Cümle 2 <-> Cümle 3 Benzerlik: 0.76\n  Cümle 3 <-> Cümle 4 Benzerlik: 0.22\n  Cümle 4 <-> Cümle 5 Benzerlik: 0.24\n  Cümle 5 <-> Cümle 6 Benzerlik: 0.82\n  Cümle 6 <-> Cümle 7 Benzerlik: 0.72\n  Cümle 7 <-> Cümle 8 Benzerlik: 0.77\n--------------------\n\n--- Oluşturulan Chunk'lar ---\nCHUNK 1:\n\"QuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\"\n(Uzunluk: 293 karakter)\n\nCHUNK 2:\n\"fdsafjsdalfj ljfasldfkjs dafklsa.\"\n(Uzunluk: 33 karakter)\n\nCHUNK 3:\n\"Teknik Detaylar Projenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\"\n(Uzunluk: 416 karakter)\n\n\n\nPrensip: '##' ile başlayan her başlık yeni bir chunk başlatır.\n\n==================================================\nSTRATEJİ: Layout-Aware Chunking (Demo)\n==================================================\n\n--- Oluşturulan Chunk'lar ---\nCHUNK 1:\n\"Projenin Amacı\n\nQuantumLeap projesinin temel amacı, kuantum bilgisayarlarını kullanarak karmaşık lojistik problemlerini çözmektir. Bu proje, özellikle büyük ölçekli tedarik zinciri ağlarındaki optimizasyon sorunlarına odaklanmaktadır. Projenin ilk fazı başarıyla tamamlanmış ve teorik modeller doğrulanmıştır.\"\n(Uzunluk: 311 karakter)\n\nCHUNK 2:\n\"Teknik Detaylar\n\nProjenin teknik detaylarına gelecek olursak, sistem Qiskit framework'ü üzerine inşa edilmiştir ve hibrit bir kuantum-klasik yaklaşım benimser. Algoritmalar, klasik bilgisayarlarda ön işlemden geçirilen verileri, kuantum işlem biriminde (QPU) analiz eder. Sonuçlar daha sonra tekrar klasik ortama aktarılarak yorumlanır. Güvenlik için de kuantum anahtar dağıtımı (QKD) protokolleri entegre edilmiştir.\"\n(Uzunluk: 417 karakter)\n\n\n\n================================================================================\nSTRATEJİLERİN KARŞILAŞTIRMALI ÖZET TABLOSU\n================================================================================\n| Kriter            | Recursive Splitting         | Semantic Chunking         | Layout-Aware Chunking     |\n|-------------------|-----------------------------|---------------------------|---------------------------|\n| Temel Fikir       | Yapısal hiyerarşiye göre böl| Anlam akışına göre böl    | Doküman formatına göre böl| \n| Sonuç Kalitesi    | Orta - Anlamı bölebilir     | Yüksek - Konu bütünlüğü   | Çok Yüksek (Yapısal Metinde)| \n| Hesaplama Yükü    | Düşük                       | Orta-Yüksek               | Düşük-Orta                | \n| Ne Zaman Kullanmalı?| Hızlı prototip, baseline    | Anlatısal metinler        | Raporlar, dokümantasyonlar| \n================================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Uçtan Uca FAISS ve Vector DB kullanımı ile RAG","metadata":{}},{"cell_type":"markdown","source":"## Aşamaların Özeti\n\nDoküman Yükleme & Chunking\nÖrnek amaçlı, her tam cümleyi bir “chunk” olarak tanımlıyoruz. Gerçek projede metin boyutuna göre sliding window, semantic chunking veya layout-aware yöntemler tercih edilebilir.\n\n* Embedding Modeli\nÇok dilli bir SentenceTransformer modeli (XLM-R tabanlı) kullanarak her chunk için vektörel temsil (embedding) oluşturuyoruz.\n\n* Vektör Veri Tabanı (FAISS)\nL2 mesafesine dayanan bir FAISS indeksi inşa ederek embedding’leri ekliyoruz.\n\n* Türkçe LLM\nHugging Face’den indirdiğimiz GPT-2 tabanlı Türkçe model ile text-generation pipeline’ı oluşturuyoruz.\n\n* Sorgu İşleme\n\n    * Kullanıcı sorusunu embedding’e dönüştürürüz.\n\n    * FAISS ile en benzer k chunk’ı buluruz.\n\n* Promp Oluşturma\nSoru + getirilen chunk’lar birleştirilip modele verilir.\n\n* Cevap Üretimi\nModelden gelen çıktı, RAG sonucu olarak sunulur.\n\nBu örnek, bir RAG boru hattının temel taşlarını içerir. Doküman sayısı, chunking stratejisi, embedding modeli, vektör veri tabanı ve LLM tercihleriniz projenizin ihtiyacına göre değiştirilebilir.","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu transformers sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:28:26.729178Z","iopub.execute_input":"2025-09-20T17:28:26.729525Z","iopub.status.idle":"2025-09-20T17:28:32.162857Z","shell.execute_reply.started":"2025-09-20T17:28:26.729500Z","shell.execute_reply":"2025-09-20T17:28:32.162057Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.12.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Gerekli kütüphaneler: pip install transformers sentence-transformers faiss-cpu numpy torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# (1) Örnek dokümanlar ve basit chunking\ndocuments = [\n    \"Türkiye'nin başkenti Ankara'dır.\",\n    \"İstanbul, Türkiye'nin en kalabalık şehridir.\",\n    \"Kapadokya, Nevşehir ilinde yer alan ünlü bir turistik bölgedir.\",\n    \"Ege Bölgesi, zeytinyağı üretimi ile ünlüdür.\"\n]\nchunks = documents  # Her dokümanı bir chunk olarak alıyoruz\n\n# (2) Embedding: sentence-transformers ile gömme oluşturma\n# Türkçe için iyi performans gösteren bir embedding modeli\nembedder = SentenceTransformer('dbmdz/bert-base-turkish-cased')\nembeddings = embedder.encode(chunks)\n\n# (3) FAISS ile vektör veritabanı hazırlama\nd = embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(np.array(embeddings))\n\n# (4) *** TÜRKÇE TEXT GENERATION MODELİ ***\n# Seçenek 1: Türkçe GPT-2 modeli (tercih edilen)\ntry:\n    model_name = \"ytu-ce-cosmos/turkish-gpt2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    print(f\"✅ Türkçe GPT-2 modeli yüklendi: {model_name}\")\nexcept:\n    # Seçenek 2: Çok dilli GPT-2 (fallback)\n    print(\"⚠️ Türkçe model bulunamadı, çok dilli model kullanılıyor...\")\n    model_name = \"microsoft/DialoGPT-medium\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Tokenizer için gerekli ayarlar\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Pipeline'ı doğru modelle kuruyoruz\ngenerator = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=50,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    top_k=50,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# (5) RAG fonksiyonu: sorguyu al, benzer chunk'ları getir, prompt oluştur, LLM'den cevap üret\ndef rag_answer(query, k=2):\n    # 5.a) Sorguyu gömme\n    query_vec = embedder.encode([query])\n    \n    # 5.b) FAISS ile en yakın k chunk'ı ara\n    distances, indices = index.search(np.array(query_vec), k)\n    retrieved = [chunks[i] for i in indices[0]]\n    context = \"\\n\".join(retrieved)\n    \n    # 5.c) Prompt'u hazırla ve cevap üret\n    prompt = f\"Bağlam: {context}\\n\\nSoru: {query}\\nCevap:\"\n    \n    print(\"--- LLM'e Giden Prompt ---\")\n    print(prompt)\n    print(\"-\" * 40)\n    \n    try:\n        result = generator(prompt, return_full_text=False)\n        answer = result[0]['generated_text'].strip()\n        \n        # Eğer cevap boş veya anlamsızsa basit bir yanıt döndür\n        if not answer or len(answer.strip()) < 5:\n            # Context'ten doğrudan yanıt çıkarma\n            if \"İstanbul\" in context and (\"büyük\" in query or \"kalabalık\" in query):\n                return \"İstanbul\"\n            elif \"Ankara\" in context and \"başkent\" in query:\n                return \"Ankara\"\n            else:\n                return \"Verilen bağlamda bu sorunun cevabı bulunmuyor.\"\n        \n        return answer\n    except Exception as e:\n        print(f\"❌ Model hatası: {e}\")\n        # Basit anahtar kelime eşleştirmesi ile cevap verme\n        if \"büyük şehir\" in query or \"kalabalık\" in query:\n            return \"İstanbul, Türkiye'nin en kalabalık şehridir.\"\n        elif \"başkent\" in query:\n            return \"Türkiye'nin başkenti Ankara'dır.\"\n        else:\n            return \"Bu sorunun cevabını veremiyorum.\"\n\n# (6) Alternatif basit RAG fonksiyonu (model çalışmazsa)\ndef simple_rag_answer(query, k=2):\n    # Sorguyu gömme\n    query_vec = embedder.encode([query])\n    \n    # En yakın chunk'ları bul\n    distances, indices = index.search(np.array(query_vec), k)\n    retrieved = [chunks[i] for i in indices[0]]\n    \n    print(\"--- Bulunan En Yakın Dokümanlar ---\")\n    for i, doc in enumerate(retrieved):\n        print(f\"{i+1}. {doc}\")\n    print(\"-\" * 40)\n    \n    # Basit anahtar kelime tabanlı cevaplama\n    context = \" \".join(retrieved).lower()\n    query_lower = query.lower()\n    \n    if any(word in query_lower for word in [\"büyük\", \"kalabalık\"]):\n        if \"istanbul\" in context:\n            return \"İstanbul, Türkiye'nin en kalabalık şehridir.\"\n    \n    if \"başkent\" in query_lower:\n        if \"ankara\" in context:\n            return \"Türkiye'nin başkenti Ankara'dır.\"\n    \n    # En yakın dokümanı döndür\n    return retrieved[0] if retrieved else \"Cevap bulunamadı.\"\n\n# (7) Örnek kullanım\nif __name__ == \"__main__\":\n    print(\"🚀 RAG Sistemi Başlatılıyor...\\n\")\n    \n    soru = \"Türkiye'nin en büyük şehri hangisidir?\"\n    print(f\"📋 Soru: {soru}\")\n    \n    try:\n        cevap = rag_answer(soru, k=2)\n    except:\n        print(\"⚠️ Ana model hatası, basit RAG kullanılıyor...\")\n        cevap = simple_rag_answer(soru, k=2)\n    \n    print(f\"✅ Cevap: {cevap}\")\n    \n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    soru_2 = \"Başkent neresidir?\"\n    print(f\"📋 Soru: {soru_2}\")\n    \n    try:\n        cevap_2 = rag_answer(soru_2, k=1)\n    except:\n        print(\"⚠️ Ana model hatası, basit RAG kullanılıyor...\")\n        cevap_2 = simple_rag_answer(soru_2, k=1)\n    \n    print(f\"✅ Cevap: {cevap_2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T17:38:36.890794Z","iopub.execute_input":"2025-09-20T17:38:36.891105Z","iopub.status.idle":"2025-09-20T17:38:47.456921Z","shell.execute_reply.started":"2025-09-20T17:38:36.891084Z","shell.execute_reply":"2025-09-20T17:38:47.456234Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba96f6503c840ac8afe38b276660e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c29fbf4773b4593853e921fb95f1fe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7ffc19141a4b86a0fe87c34d417013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b363df35754bd98c2e25c671dc0385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80756b0d15004d8890051369664e88b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab0448354944bcda06ab1ff5a455706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/893 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de663493c52241e0950e290b87e75fb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec418c90864945d2a1584dc24c6d5a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3306d29ca6424323b1d3970083c5055a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"✅ Türkçe GPT-2 modeli yüklendi: ytu-ce-cosmos/turkish-gpt2\n🚀 RAG Sistemi Başlatılıyor...\n\n📋 Soru: Türkiye'nin en büyük şehri hangisidir?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092aa18381cc47d6a51190b2ab1aec2a"}},"metadata":{}},{"name":"stdout","text":"--- LLM'e Giden Prompt ---\nBağlam: İstanbul, Türkiye'nin en kalabalık şehridir.\nTürkiye'nin başkenti Ankara'dır.\n\nSoru: Türkiye'nin en büyük şehri hangisidir?\nCevap:\n----------------------------------------\n✅ Cevap: İstanbul, Türkiye'nin en kalabalık şehridir.\n\nSoru: Türkiye'nin en büyük şehri hangisidir?\nCevap: İstanbul, Türkiye'nin en büyük şehridir.\nSoru: Türkiye'nin en büyük şehri hangisidir?\nCevap: İstanbul\n\n==================================================\n\n📋 Soru: Başkent neresidir?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73dea7c6cac54904b3448e75fa294499"}},"metadata":{}},{"name":"stdout","text":"--- LLM'e Giden Prompt ---\nBağlam: Türkiye'nin başkenti Ankara'dır.\n\nSoru: Başkent neresidir?\nCevap:\n----------------------------------------\n✅ Cevap: Başkent, Ankara'nın kuzeydoğusunda yer alan bir ilçedir.\nAnkara'nın kuzeyinde yer alan Çubuk, kuzeyinde Yenimahalle, doğusunda Keçiören, doğusunda Keçiören ve doğusunda ise Çankaya ilçeleri yer almaktadır.\nAnkara'nın kuzeydoğusunda yer alan Çukurambar,\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}